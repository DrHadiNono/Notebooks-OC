{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# from pathlib import Path\n",
    "# import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "production_mode = False\n",
    "local_mode = True\n",
    "OUTPUT_IMG = False\n",
    "NB_CHANNEL = 1 # 1:grayscale, 3:RGB\n",
    "REDUCTION_FACTOR = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext()\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "origin_path = 'F:/' if local_mode else 's3a://dr.hadinono/OC/P8/'\n",
    "\n",
    "img_dir = origin_path+'fruits-360/Training/*'\n",
    "if not production_mode:\n",
    "    img_dir = origin_path+'fruits-360/Training/Apple_Braeburn/'\n",
    "output_dir = origin_path+'fruits-360/PreprocessedTraining/'\n",
    "csv_dir = origin_path+'fruits-360/CSV/Separate/'\n",
    "  \n",
    "# if local_mode and production_mode:\n",
    "#     # Create the data folders to store the outputs\n",
    "#     for name in os.listdir(img_dir[:-2]):\n",
    "#         if OUTPUT_IMG:\n",
    "#             Path(output_dir+name).mkdir(parents=True, exist_ok=True)\n",
    "#         Path(csv_dir+name).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(image):\n",
    "    filename = image[0]\n",
    "    filedir = filename.split('/')[0]\n",
    "\n",
    "    # Save preprocessed image\n",
    "    # to JPG\n",
    "    img = image[1]\n",
    "    if OUTPUT_IMG:\n",
    "        new_img = Image.fromarray(img.astype('uint8'), 'RGB' if NB_CHANNEL==3 else 'L')\n",
    "        new_img.save(output_dir+filename)\n",
    "        del(new_img)\n",
    "\n",
    "    # to CSV with label\n",
    "    # df = pd.concat(\n",
    "    #     [pd.DataFrame({'label': [filedir]}), pd.DataFrame(img.flatten()).T], axis=1)\n",
    "    df = pd.DataFrame({'label': [filedir], 'features':[DenseVector(img.flatten())]})\n",
    "    df.to_csv(csv_dir+filename.split('.')[0]+'.csv', index=False, sep=\";\" , quoting=3)\n",
    "\n",
    "    del(img)\n",
    "    del(df)\n",
    "\n",
    "\n",
    "def normalize(arr):\n",
    "    \"\"\"\n",
    "    Linear normalization\n",
    "    http://en.wikipedia.org/wiki/Normalization_%28image_processing%29\n",
    "    \"\"\"\n",
    "    arr = arr.astype('float')\n",
    "    minval = None\n",
    "    maxval = None\n",
    "    if NB_CHANNEL == 1 :\n",
    "        arr = arr.flatten()\n",
    "        arr *= (255.0/(arr.max()-arr.min()))\n",
    "        arr = np.resize(arr, (100//REDUCTION_FACTOR, 100//REDUCTION_FACTOR))\n",
    "    else:\n",
    "        for i in range(NB_CHANNEL):\n",
    "            minval = arr[..., i].min()\n",
    "            maxval = arr[..., i].max()\n",
    "            if minval != maxval:\n",
    "                arr[..., i] -= minval\n",
    "                arr[..., i] *= (255.0/(maxval-minval))\n",
    "    del(minval)\n",
    "    del(maxval)\n",
    "    return arr\n",
    "\n",
    "def reduce_image(img):\n",
    "    img = Image.fromarray(img.astype('uint8'), 'RGB')\n",
    "    if NB_CHANNEL==1 :\n",
    "        img = img.convert('L')\n",
    "    img = img.resize((100//REDUCTION_FACTOR, 100//REDUCTION_FACTOR), Image.ANTIALIAS) \n",
    "    return np.array(img)\n",
    "\n",
    "def image_to_array(img):\n",
    "    filename = '/'.join(img.origin.split('/')[-2:])\n",
    "    img = np.resize(np.asarray(list(img.data)), (100,100, 3))\n",
    "    img = reduce_image(img)\n",
    "    img = normalize(img)\n",
    "    save((filename, img))\n",
    "    # There is nothing to return, because there is no collect. All the processing and the exports is done for each image separately by each executors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = spark.read.format(\"image\").load(img_dir).select(\"image.origin\", \"image.data\")\n",
    "if not production_mode:\n",
    "    imgs = imgs.limit(5)\n",
    "imgs = imgs.rdd.map(image_to_array).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>> all done!\n"
     ]
    }
   ],
   "source": [
    "# Close Spark\n",
    "print('>>>>>>>> all done!')\n",
    "sc.stop()\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "22a000a366ddd64c4ddab40e3652306b9e13e369594fde0c1e9f1fab224351e6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
